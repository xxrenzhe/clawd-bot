---
title: "Moltbot Streaming: Real-Time AI Responses [2026]"
description: "Enable streaming responses in Moltbot. Configure real-time output, SSE endpoints, chunked delivery, and low-latency response setup."
pubDate: 2026-01-27
modifiedDate: 2026-01-29
category: "Advanced"
tags: ["moltbot","clawdbot","streaming","real-time","responses","performance"]
keywords: ["moltbot streaming","clawdbot real-time","streaming responses","sse","low latency ai", "openclaw"]
readingTime: 7
featured: false
author: "Moltbot Team"
image: "/images/articles/clawdbot-streaming-responses.jpg"
imageAlt: "Moltbot Streaming Responses"
articleType: "TechArticle"
difficulty: "intermediate"
sources:
  - "https://docs.molt.bot/"
  - "https://github.com/moltbot/moltbot"
relatedArticles:
  - "clawdbot-performance-tuning"
  - "clawdbot-api-integration"
  - "clawdbot-rate-limiting"
---
import HostingCTA from '../../components/CTA/HostingCTA.astro';

# Moltbot Streaming: Real-Time AI Responses

![Moltbot Streaming Responses](/images/articles/clawdbot-streaming-responses.jpg)

## Streaming vs Buffered

| Mode | First Token | Complete Response | Best For |
|------|-------------|-------------------|----------|
| Streaming | ~200ms | Progressive | Chat, long responses |
| Buffered | 2-10s | All at once | Short tasks, batch |

<HostingCTA context="setup" />

## Enable Streaming

```json
{
  "agent": {
    "streaming": {
      "enabled": true,
      "chunkSize": "token",
      "flushInterval": 50
    }
  }
}
```

<HostingCTA context="inline" />

## Channel-Specific Settings

### Telegram

```json
{
  "channels": {
    "telegram": {
      "streaming": {
        "enabled": true,
        "updateInterval": 500,
        "showTyping": true
      }
    }
  }
}
```

Telegram shows "typing..." indicator while streaming, then updates message every 500ms.

### Discord

```json
{
  "channels": {
    "discord": {
      "streaming": {
        "enabled": true,
        "editMessage": true,
        "updateInterval": 1000
      }
    }
  }
}
```

Discord edits the response message as content arrives.

### Web/API

```json
{
  "gateway": {
    "api": {
      "streaming": {
        "format": "sse",
        "keepAlive": 30000
      }
    }
  }
}
```

## SSE (Server-Sent Events)

### Endpoint

```
GET /api/chat/stream
```

### Request

```bash
curl -N "http://localhost:18789/api/chat/stream" \
  -H "Content-Type: application/json" \
  -d '{"message": "Explain quantum computing"}'
```

### Response Stream

```
event: start
data: {"id": "msg_123", "model": "claude-3-5-sonnet"}

event: delta
data: {"content": "Quantum"}

event: delta
data: {"content": " computing"}

event: delta
data: {"content": " is"}

...

event: done
data: {"usage": {"input": 5, "output": 234}}
```

## JavaScript Client

```javascript
const eventSource = new EventSource('/api/chat/stream?message=Hello');

eventSource.addEventListener('delta', (event) => {
  const data = JSON.parse(event.data);
  document.getElementById('response').textContent += data.content;
});

eventSource.addEventListener('done', (event) => {
  eventSource.close();
  console.log('Complete:', JSON.parse(event.data));
});

eventSource.onerror = (error) => {
  console.error('Stream error:', error);
  eventSource.close();
};
```

## Python Client

```python
import requests

def stream_response(message):
    response = requests.get(
        'http://localhost:18789/api/chat/stream',
        params={'message': message},
        stream=True
    )

    for line in response.iter_lines():
        if line:
            decoded = line.decode('utf-8')
            if decoded.startswith('data: '):
                data = json.loads(decoded[6:])
                if 'content' in data:
                    print(data['content'], end='', flush=True)

stream_response("What is machine learning?")
```

## Streaming Modes

### Token-by-Token

```json
{
  "agent": {
    "streaming": {
      "chunkSize": "token"
    }
  }
}
```

Most responsive, sends each token as generated.

### Word Chunks

```json
{
  "agent": {
    "streaming": {
      "chunkSize": "word",
      "minChunkSize": 3
    }
  }
}
```

Batches tokens into words, reduces network overhead.

### Sentence Chunks

```json
{
  "agent": {
    "streaming": {
      "chunkSize": "sentence"
    }
  }
}
```

Waits for complete sentences, smoother reading experience.

## Buffering Options

### Minimum Buffer

```json
{
  "agent": {
    "streaming": {
      "buffer": {
        "minTokens": 10,
        "maxWait": 200
      }
    }
  }
}
```

### Flush Control

```json
{
  "agent": {
    "streaming": {
      "flush": {
        "onNewline": true,
        "onPunctuation": true,
        "interval": 100
      }
    }
  }
}
```

## Tool Use with Streaming

When tools are called during streaming:

```json
{
  "agent": {
    "streaming": {
      "tools": {
        "showProgress": true,
        "pauseOnCall": false
      }
    }
  }
}
```

Example output:
```
You: What's the weather in Tokyo?

Moltbot: I'll check the weather for you.

[üîç Calling get_weather...]

The current weather in Tokyo is 18¬∞C with partly cloudy skies...
```

## Connection Management

### Keep-Alive

```json
{
  "gateway": {
    "streaming": {
      "keepAlive": {
        "enabled": true,
        "interval": 15000,
        "message": ":ping"
      }
    }
  }
}
```

### Timeout Settings

```json
{
  "gateway": {
    "streaming": {
      "timeout": {
        "idle": 60000,
        "total": 300000
      }
    }
  }
}
```

### Reconnection

```json
{
  "gateway": {
    "streaming": {
      "reconnect": {
        "enabled": true,
        "maxAttempts": 3,
        "backoff": "exponential"
      }
    }
  }
}
```

## Performance Metrics

```bash
moltbot streaming stats
```

Output:
```
üìä Streaming Statistics (Last 24h)

Connections: 1,234
‚îú‚îÄ Active: 12
‚îú‚îÄ Completed: 1,189
‚îî‚îÄ Failed: 33 (2.7%)

Latency:
‚îú‚îÄ Time to first token: 180ms (avg)
‚îú‚îÄ Token interval: 45ms (avg)
‚îî‚îÄ Total duration: 3.2s (avg)

Throughput:
‚îú‚îÄ Tokens streamed: 456,789
‚îú‚îÄ Bytes sent: 2.1 MB
‚îî‚îÄ Messages: 1,222
```

## Provider-Specific Notes

### Anthropic (Claude)

```json
{
  "agent": {
    "providers": {
      "anthropic": {
        "streaming": true,
        "streamEvents": ["content_block_delta", "message_stop"]
      }
    }
  }
}
```

### OpenAI

```json
{
  "agent": {
    "providers": {
      "openai": {
        "streaming": true,
        "streamOptions": {
          "includeUsage": true
        }
      }
    }
  }
}
```

### Ollama (Local)

```json
{
  "agent": {
    "providers": {
      "ollama": {
        "streaming": true,
        "host": "http://localhost:11434"
      }
    }
  }
}
```

## Disable Streaming

For specific use cases:

```json
{
  "agent": {
    "streaming": {
      "enabled": true,
      "disableFor": [
        "code_execution",
        "file_operations",
        "short_responses"
      ]
    }
  }
}
```

## Troubleshooting

| Issue | Cause | Solution |
|-------|-------|----------|
| No stream output | Buffering too high | Reduce flushInterval |
| Choppy updates | Network latency | Increase updateInterval |
| Connection drops | Timeout too short | Increase idle timeout |
| Missing tokens | Client not handling SSE | Check event listener |
| High latency | Proxy buffering | Disable proxy buffering |

### Debug Streaming

```bash
# Test stream directly
moltbot chat "Hello" --stream --debug

# Check SSE format
curl -N "http://localhost:18789/api/chat/stream?message=test" --verbose
```

### Proxy Configuration

If using nginx:
```nginx
location /api/chat/stream {
    proxy_pass http://localhost:18789;
    proxy_buffering off;
    proxy_cache off;
    proxy_set_header Connection '';
    proxy_http_version 1.1;
    chunked_transfer_encoding off;
}
```

## Next Steps

- [Performance tuning](/articles/clawdbot-performance-tuning)
- [API integration](/articles/clawdbot-api-integration)
- [Rate limiting](/articles/clawdbot-rate-limiting)

<HostingCTA context="conclusion" />
