---
title: "Running Local LLMs with Moltbot: Ollama and Open Source Models"
description: "Unlock privacy and control by running local LLMs with Moltbot and Ollama. This tutorial guides you through setting up your private AI assistant using open source models."
pubDate: 2026-02-01
modifiedDate: 2026-02-01
category: "Tutorial"
tags: ["local llm","ollama","open source llm","moltbot local models","private ai"]
keywords: ["local llm","ollama","open source llm","moltbot local models","private ai","openclaw","moltbot","clawdbot"]
readingTime: 12
featured: false
author: "Moltbot Team"
image: "/images/articles/running-local-llms-with-moltbot-ollama-and-open-source-model.jpg"
imageAlt: "Running Local LLMs with Moltbot: Ollama and Open Source Models"
articleType: "HowTo"
difficulty: "beginner"
sources:
  - "https://docs.molt.bot/"
  - "https://github.com/clawdbot/clawdbot"
---

## Reclaiming Control: Why Local LLMs are the Future of Personal AI

The AI landscape is shifting, and rapidly. We're seeing massive deals stall, concerns over data privacy escalating, and a growing desire among developers and users alike to move beyond monolithic, cloud-dependent AI services. You've probably felt it too â€“ the allure of powerful LLMs like Claude and GPT-4, tempered by questions around cost, data ownership, and the inherent black box nature of proprietary APIs.

The truth is, while commercial LLMs offer incredible capabilities, they often come with tradeoffs. Your data travels to external servers, costs can quickly accumulate, and you're always at the mercy of provider policies and service uptime. What if you could have the power of an AI assistant, deeply integrated into your daily workflow, with the assurance that your data stays **private**, on **your own hardware**, running **open-source models**?

This isn't just a pipe dream. It's becoming a reality thanks to innovative tools like Moltbot, paired with the growing ecosystem of local LLMs facilitated by platforms like Ollama. In this comprehensive guide, you're going to learn how to set up your very own **private AI assistant** using Moltbot (also known as Clawdbot or Openclaw) and an **open source LLM** running locally via Ollama. This approach gives you unparalleled control, privacy, and cost-effectiveness.

<HostingCTA context="setup" />

## Moltbot: Your Personal AI Gateway to Open Source Power

Moltbot is an open-source, self-hosted personal AI assistant designed to bridge the gap between your favorite chat applications (Telegram, Discord, Slack, WhatsApp) and powerful Large Language Models. While it seamlessly integrates with commercial APIs like Claude and GPT-4, its true power for those seeking sovereignty lies in its ability to connect with **local LLMs**.

Here's why Moltbot is the perfect companion for your local LLM journey:

*   **Self-hosted & Private:** Your data never leaves your machine. This is paramount for sensitive information and personal projects.
*   **Persistent Memory:** Unlike many stateless LLM interactions, Moltbot provides file-based memory persistence. Your AI remembers past conversations and preferences, building a richer, more context-aware relationship with you over time.
*   **Multi-platform Messaging:** Interact with your AI via the messaging app you already use every day.
*   **Extensible Skills & Plugins:** Tailor your AI's capabilities with custom tools, allowing it to perform proactive automation and even have full computer access (with proper security configurations, of course!).
*   **Flexibility with LLMs:** While we're focusing on Ollama today, Moltbot's architecture makes it easy to swap out LLMs or even run multiple in parallel.

Our goal today is to combine Moltbot's robust gateway features with the accessibility of Ollama to run a powerful **open-source LLM** right on your machine. Let's dive in!

## Step 1: Getting Your Local LLM Ready with Ollama

Before we bring Moltbot into the picture, we need a local LLM to connect to. This is where Ollama shines. Ollama allows you to run large language models on your local machine, simplifying the setup and management of various open-source models.

### What is Ollama?

Think of Ollama as a user-friendly wrapper for running LLMs locally. It provides a simple CLI to pull, run, and manage models, and exposes a compatible API endpoint that Moltbot can easily connect to.

### Installing Ollama

First things first, let's get Ollama installed.

1.  **Download Ollama:** Visit the official Ollama website: [https://ollama.com/](https://ollama.com/)
2.  **Select Your OS:** Download the appropriate installer for your operating system (macOS, Linux, Windows).
3.  **Install:** Follow the installation instructions for your OS. It's usually a straightforward process.

### Pulling Your First Open Source LLM

Once Ollama is installed, you can easily download and run an LLM. For this tutorial, we'll use `llama2`, a popular and capable open-source model.

Open your terminal or command prompt and run:

```bash
ollama pull llama2
```

This command will download the `llama2` model. Depending on your internet speed and the model size, this might take a few minutes.

Once downloaded, you can test it directly in Ollama:

```bash
ollama run llama2
```

You should now be able to chat directly with `llama2` in your terminal. This confirms Ollama is working correctly. Type `"/bye"` to exit the chat.

**Pro Tip:** Ollama also has a web API that runs in the background (usually on `http://localhost:11434`). Moltbot will use this API to communicate with your local LLMs. Ensure Ollama is running in the background (it typically starts automatically after installation).

## Step 2: Installing and Onboarding Moltbot

Now that our local LLM is ready, let's get Moltbot installed and configured.

### Quick Install Moltbot

Moltbot provides a convenient one-liner for installation:

Open your terminal and run:

```bash
curl -fsSL https://clawd.bot/install.sh | bash
```

This script will download and set up Moltbot on your system.

### Onboarding Your Personal AI

Once installed, it's time to onboard Moltbot. This guided process helps set up your initial configuration.

```bash
clawdbot onboard
```

During the onboarding, you'll be prompted for a few things:

*   **API Key:** For local LLMs, you won't need an external API key initially. You can typically skip this or use a placeholder if required, as we'll explicitly configure Ollama later.
*   **Model Selection:** You'll be asked about your preferred model. This is where you'll tell Moltbot you want to use a local model via Ollama.
*   **Channel Selection:** Choose a messaging platform (e.g., Telegram, Discord). We'll go into more detail on connecting a channel in a later step.

Follow the prompts. Don't worry if you don't get the configuration *perfect* during onboarding; we'll refine it manually in the next step.

You can always check Moltbot's health after installation:

```bash
clawdbot health
```

This command will verify that Moltbot's core components are running correctly.

## Step 3: Configuring Moltbot to Use Ollama (The Heart of Your Private AI)

This is the most critical step to get your **local LLM** working with Moltbot. Moltbot's configuration is managed through a YAML file, typically located at `~/.config/clawdbot/config.yaml`.

Open this file with your favorite text editor (e.g., `nano ~/.config/clawdbot/config.yaml` or `code ~/.config/clawdbot/config.yaml`).

We need to define Ollama as a model provider and then specify which Ollama model Moltbot should use.

### Adding Ollama to Your Moltbot Configuration

Locate the `models:` section in your `config.yaml`. If it doesn't exist, create it. Add an entry for your Ollama setup like this:

```yaml
# ~/.config/clawdbot/config.yaml

gateway:
  # ... other gateway settings ...

models:
  # ... existing models (if any) ...

  ollama-llama2-local:
    provider: ollama
    endpoint: http://localhost:11434/api/generate
    model: llama2
    # Optional: You can also specify an embeddings model if you want
    # embeddings_model: ollama-nomic-embed-text # Requires 'ollama pull nomic-embed-text'
    # embeddings_endpoint: http://localhost:11434/api/embeddings
```

Let's break down this configuration:

*   `ollama-llama2-local`: This is an arbitrary name *you choose* for this specific model configuration within Moltbot. Make it descriptive!
*   `provider: ollama`: This tells Moltbot to use its built-in Ollama integration.
*   `endpoint: http://localhost:11434/api/generate`: This is the default API endpoint where Ollama serves its models. Make sure Ollama is running in the background for this to work.
*   `model: llama2`: This specifies which Ollama model Moltbot should request. It *must* match the name of a model you've `ollama pull`ed (e.g., `llama2`, `mistral`, `phi3`).

### Setting Your Default Model

After defining your Ollama model, you'll likely want to set it as the default for your conversations. Find the `default_model:` setting (or add it if missing) and point it to your new Ollama configuration name:

```yaml
# ~/.config/clawdbot/config.yaml

default_model: ollama-llama2-local

models:
  # ... your Ollama configuration from above ...
```

Save and close the `config.yaml` file.

<HostingCTA context="inline" />

## Step 4: Connecting a Messaging Channel (e.g., Telegram)

Moltbot is designed to integrate with your existing chat applications. For this tutorial, we'll walk through setting up Telegram, but the process is similar for Discord, Slack, or WhatsApp.

### Setting Up a Telegram Bot

1.  **Open Telegram and search for `@BotFather`**. This is Telegram's official bot for creating other bots.
2.  **Start a chat** with `@BotFather`.
3.  **Type `/newbot`** and follow the instructions to choose a name and a username for your bot.
4.  **BotFather will provide you with an HTTP API token.** This is crucial! Copy this token. It will look something like `123456:ABC-DEF1234ghIkl-zyx57W213423`.

### Adding Telegram to Moltbot Configuration

Now, open your `config.yaml` again (`~/.config/clawdbot/config.yaml`). Find the `channels:` section (or create it) and add your Telegram bot's configuration:

```yaml
# ~/.config/clawdbot/config.yaml

channels:
  telegram:
    token: "YOUR_TELEGRAM_BOT_TOKEN_HERE" # Paste the token from BotFather
    policy: direct_message # Or 'pairing' for more control, see Security Best Practices
    # Optional: If you want specific Telegram chats to use a specific model
    # model: ollama-llama2-local
```

Replace `"YOUR_TELEGRAM_BOT_TOKEN_HERE"` with the actual token you received from BotFather.

**Policy Explanation:**
*   `direct_message`: The bot will respond to all direct messages from any user.
*   `pairing`: Requires you to explicitly approve new devices/users before they can interact with your bot. This is a recommended security measure for personal bots.

Save and close the `config.yaml` file.

## Step 5: Running the Moltbot Gateway and Testing Your Setup

With Moltbot configured to use Ollama and connected to your Telegram bot, it's time to bring everything online.

### Start the Moltbot Gateway

In your terminal, run the Moltbot gateway command:

```bash
clawdbot gateway --port 18789 --verbose
```

*   `--port 18789`: This specifies the port Moltbot will use for its internal API (not directly relevant for chat integrations, but good practice).
*   `--verbose`: This will show detailed logs in your terminal, which is incredibly helpful for troubleshooting.

You should see output indicating that Moltbot is starting up, connecting to your specified channels, and loading your models. Look for messages about the `telegram` channel being active and your `ollama-llama2-local` model being available.

### Test Your Private AI Assistant!

1.  **Open Telegram.**
2.  **Find your newly created bot** by its username.
3.  **Start a chat** with it.
4.  **Send a message!** Try something simple like "Hello, who are you?" or "Tell me a fun fact about AI."

**Congratulations!** You now have Moltbot running with a **local LLM** via Ollama. You're chatting with an **open source model** on your own hardware, with your data staying completely private.

### Troubleshooting Tips:

*   **"Error connecting to Ollama" / "Model not found":**
    *   Ensure Ollama is running in the background. Check `http://localhost:11434` in your browser.
    *   Verify the `endpoint` in `config.yaml` is correct.
    *   Confirm you've `ollama pull`ed the model (`llama2` in our example) and that the `model:` name in `config.yaml` exactly matches.
*   **"Telegram bot not responding":**
    *   Double-check your Telegram bot token in `config.yaml`.
    *   Ensure your internet connection is active.
    *   Look at the Moltbot gateway logs for any error messages related to Telegram.
*   **Permissions:** If you're on Linux, ensure the user running Moltbot has necessary file permissions to create the config file and log directories.

## Security Best Practices for Your Self-Hosted AI

Running a self-hosted AI gateway like Moltbot offers immense privacy benefits, but it also means you're responsible for its security. Here are crucial best practices to ensure your **private AI** remains truly private:

*   **Bind Gateway to Loopback (`gateway.bind`):** By default, Moltbot's gateway might bind to `0.0.0.0`, making it accessible from other machines on your network. For a truly personal, private setup, bind it to `127.0.0.1` (loopback) to prevent external exposure.

    ```yaml
    gateway:
      bind: "127.0.0.1" # Crucial for local-only access
      port: 18789
    ```

*   **Use "Pairing" Mode for DM Policy:** For your messaging channels (like Telegram), the `pairing` policy is much more secure than `direct_message`. In `pairing` mode, Moltbot will only respond to devices you've manually approved, preventing unauthorized access to your AI.

    ```yaml
    channels:
      telegram:
        token: "YOUR_TELEGRAM_BOT_TOKEN"
        policy: pairing # Use this for security
    ```

    When using `pairing`, Moltbot will output a pairing code in its logs when a new user tries to message it. You then need to manually approve that user via a Moltbot command (check the documentation for the exact command, usually `clawdbot approve <device_id>`).

*   **Configure Gateway Authentication (`gateway.auth.password`):** If you *do* need to expose Moltbot's gateway to your local network (e.g., for other local applications to use it), always secure it with a password.

    ```yaml
    gateway:
      auth:
        password: "your_strong_secret_password"
    ```

*   **Trusted Proxies (`gateway.trustedProxies`):** If you're running Moltbot behind a reverse proxy (like Nginx or Caddy) for advanced setups or SSL termination, configure `gateway.trustedProxies` to prevent IP spoofing.

    ```yaml
    gateway:
      trustedProxies:
        - "192.168.1.100" # IP address of your reverse proxy
    ```

*   **Sandbox Tools for Group Chats:** If you enable tools or plugins that grant Moltbot computer access, exercise extreme caution in group chats. Ensure these tools are adequately sandboxed or disabled for public-facing interactions to prevent malicious use or data leakage.

By following these best practices, you can enjoy the full power and privacy of your self-hosted **Clawdbot** with peace of mind.

## Free Moltbot Installation Service

We understand that setting up complex technical environments can sometimes be challenging. That's why we offer a free Moltbot installation service. Get started at [Contact](/contact). We're here to help you unlock the full potential of your private AI assistant.

## Conclusion: Embrace Your Private, Open Source AI Journey

You've done it! You've successfully installed Moltbot, integrated it with Ollama, and are now running a powerful **open source LLM** directly on your machine. This setup is more than just a cool project; it's a statement about **privacy**, **control**, and the future of **personal AI**. You're no longer reliant on external APIs for every interaction, and your data stays exactly where it belongs: with you.

This foundation opens up a world of possibilities:

*   **Experiment with other Ollama models:** Pull `Mistral`, `Phi-3`, `Llama3`, or other specialized models with `ollama pull <model_name>` and update your Moltbot config.
*   **Explore Moltbot's memory features:** Dive into how Moltbot uses file-based memory to give your AI long-term context.
*   **Build custom plugins:** Extend Moltbot's capabilities with your own tools for proactive automation, connecting it to your smart home, calendar, or other services.
*   **Integrate with other channels:** Set up Discord, Slack, or WhatsApp to interact with your **Openclaw** from anywhere.

The journey into self-hosted AI is just beginning, and with **Moltbot** as your gateway to **local LLMs**, you're at the forefront. Enjoy the power of truly private, intelligent assistance!

<HostingCTA context="conclusion" />